{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9b10baa",
   "metadata": {},
   "source": [
    "# CLIP-guided StyleGAN2 image generation\n",
    "\n",
    "This notebook is a direct conversion of `generate_images_from_prompt.py`.\n",
    "\n",
    "It performs CLIP-guided latent optimization on a StyleGAN2 generator (.pkl)\n",
    "to create an image matching a text prompt.\n",
    "\n",
    "Usage notes:\n",
    "- Provide a local StyleGAN2 `.pkl` (NVIDIA/stylegan2-ada format) and set `pkl_path` in the config cell.\n",
    "- Ensure the stylegan2-ada `legacy.py` and `dnnlib` are on your PYTHONPATH.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c31c946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install (uncomment to run in notebook)\n",
    "# !pip install -r ../src/requirements.txt\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import clip\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display\n",
    "\n",
    "# device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07a3a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook configuration (replace values as needed)\n",
    "prompt = 'a fantasy castle on a cliff at sunset'\n",
    "pkl_path = '/path/to/stylegan2-ffhq.pkl'  # <- set to your local checkpoint\n",
    "outdir = Path('outputs')\n",
    "steps = 300\n",
    "lr = 0.1\n",
    "seed = 42\n",
    "\n",
    "outdir.mkdir(parents=True, exist_ok=True)\n",
    "torch.manual_seed(seed)\n",
    "print('Outdir:', outdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d921809",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading CLIP model...')\n",
    "clip_model, clip_preprocess = clip.load('ViT-B/32', device=device)\n",
    "clip_model.eval()\n",
    "print('CLIP loaded on', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edd5c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stylegan2_g(pkl_path, device):\n",
    "    try:\n",
    "        import dnnlib\n",
    "        import legacy\n",
    "    except Exception:\n",
    "        raise RuntimeError(\n",
    "            'Missing stylegan2-ada loader dependencies.\\n'\n",
    "            'Clone https://github.com/NVlabs/stylegan2-ada-pytorch and ensure legacy.py and dnnlib are on PYTHONPATH.'\n",
    "        )\n",
    "    with open(pkl_path, 'rb') as f:\n",
    "        G = legacy.load_network_pkl(f)['G_ema'].to(device)\n",
    "    G.eval()\n",
    "    return G\n",
    "\n",
    "# Usage example (uncomment and set pkl_path):\n",
    "# G = load_stylegan2_g(pkl_path, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb58dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synth_image_from_w(G, w):\n",
    "    with torch.no_grad():\n",
    "        if w.ndim == 2:\n",
    "            w_in = w.unsqueeze(1).repeat(1, G.num_ws, 1)\n",
    "        else:\n",
    "            w_in = w\n",
    "        img = G.synthesis(w_in, noise_mode='const')\n",
    "    img = (img.clamp(-1, 1) + 1) / 2\n",
    "    return img\n",
    "\n",
    "\n",
    "def preprocess_for_clip(img_tensor):\n",
    "    pil = transforms.ToPILImage()(img_tensor.squeeze(0).cpu())\n",
    "    img = transforms.Resize((224, 224))(pil)\n",
    "    img = transforms.ToTensor()(img).unsqueeze(0)\n",
    "    return img.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbb0917",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optimization(prompt, pkl_path, outdir, steps=300, lr=0.1, seed=42, device=device, display_every=50):\n",
    "    torch.manual_seed(seed)\n",
    "    clip_model.eval()\n",
    "    G = load_stylegan2_g(pkl_path, device)\n",
    "\n",
    "    # initialize latent\n",
    "    z = torch.randn(1, G.z_dim, device=device)\n",
    "    with torch.no_grad():\n",
    "        w = G.mapping(z, None)\n",
    "    w_opt = w.detach().clone()\n",
    "    w_opt.requires_grad = True\n",
    "\n",
    "    # text feature\n",
    "    text_tokens = clip.tokenize([prompt]).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_feat = clip_model.encode_text(text_tokens)\n",
    "        text_feat = text_feat / text_feat.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    optimizer = torch.optim.Adam([w_opt], lr=lr)\n",
    "\n",
    "    best_score = float('-inf')\n",
    "    best_img = None\n",
    "\n",
    "    for i in range(steps):\n",
    "        optimizer.zero_grad()\n",
    "        img = synth_image_from_w(G, w_opt)\n",
    "        clip_in = preprocess_for_clip(img)\n",
    "        image_feat = clip_model.encode_image(clip_in)\n",
    "        image_feat = image_feat / image_feat.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        similarity = (image_feat @ text_feat.T).squeeze()\n",
    "        loss = -similarity\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        score = float(similarity.item())\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_img = img.detach().cpu()\n",
    "\n",
    "        if (i + 1) % display_every == 0 or i == steps - 1:\n",
    "            print(f\"step {i+1}/{steps} score={score:.4f} best={best_score:.4f}\")\n",
    "            if best_img is not None:\n",
    "                display(Image.fromarray((best_img.squeeze(0) * 255).clamp(0, 255).permute(1, 2, 0).numpy().astype('uint8')))\n",
    "\n",
    "    out_path = Path(outdir) / f\"result_prompt_{seed}.png\"\n",
    "    img_to_save = (best_img.squeeze(0) * 255).clamp(0, 255).permute(1, 2, 0).numpy().astype('uint8')\n",
    "    Image.fromarray(img_to_save).save(out_path)\n",
    "    print(f\"Saved best image (score={best_score:.4f}) to {out_path}\")\n",
    "    return out_path, best_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570ee4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example run (end-to-end)\n",
    "if not Path(pkl_path).exists():\n",
    "    print('Please set `pkl_path` to a valid StyleGAN2 .pkl file and re-run this cell.')\n",
    "else:\n",
    "    out_path, score = run_optimization(prompt, pkl_path, outdir, steps=steps, lr=lr, seed=seed, device=device, display_every=max(1, steps//10))\n",
    "    print('Done:', out_path, 'score=', score)\n",
    "    display(Image.open(out_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7660e417",
   "metadata": {},
   "source": [
    "**Notes**:\n",
    "\n",
    "- This notebook requires a StyleGAN2 `.pkl` checkpoint in NVIDIA/stylegan2-ada format (contains `G_ema`).\n",
    "- If you don't have `legacy.py` and `dnnlib` available, clone https://github.com/NVlabs/stylegan2-ada-pytorch and add it to `PYTHONPATH`.\n",
    "- For faster runs use a GPU-enabled container and reduce `steps` for quick previews.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
